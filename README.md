# ðŸš€ Space Station Object Detection

This repository contains the complete pipeline for training, evaluating, and running inference on a YOLOv8 model to detect critical space station equipment (ðŸ”§ Toolbox, ðŸ›¢ï¸ OxygenTank, ðŸ”¥ FireExtinguisher) using synthetic data from Duality AIâ€™s Falcon platform.

---

## ðŸ“– Table of Contents
1. [âš™ï¸ Environment Setup](#environment-setup)  
2. [ðŸ“‚ Repository Structure](#repository-structure)  
3. [ðŸ“Š Data Preparation](#data-preparation)  
4. [ðŸ‹ï¸ Training the Model](#training-the-model)  
5. [ðŸ“ˆ Evaluating the Model](#evaluating-the-model)  
6. [ðŸ” Running Inference](#running-inference)  
7. [ðŸ”„ Reproducing Final Results](#reproducing-final-results)  
8. [ðŸ“¤ Expected Outputs](#expected-outputs)  
9. [ðŸ§  Interpreting Results](#interpreting-results)  

---

## âš™ï¸ Environment Setup

1. **Clone the repository**  
git clone https://github.com/yourusername/space-station-object-detection.git
cd space-station-object-detection

2. **Create a virtual environment** (ðŸ”§ optional but recommended)  
python3 -m venv venv
source venv/bin/activate # Linux/Mac
venv\Scripts\activate # Windows

3. **Install dependencies**
pip install -r requirements.txt

> ðŸ’¡ **Note:** Requires Python â‰¥3.8 and PyTorch with CUDA support for GPU acceleration.

---

## ðŸ“‚ Repository Structure
```text
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ðŸ“ yolo_params.yaml
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ðŸ‹ï¸ train.py
â”‚ â”œâ”€â”€ðŸ” predict.py
â”‚ â””â”€â”€ðŸŽ¨ visualize.py
â”œâ”€â”€ weights/
â”‚ â”œâ”€â”€ðŸŽ¯ best.pt
â”‚ â””â”€â”€ðŸ last.pt
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ðŸ“Š training_results.png
â”‚ â”œâ”€â”€ðŸŽ¯ confusion_matrix.png
â”‚ â”œâ”€â”€ðŸ“‘ results.csv
â”‚ â””â”€â”€ðŸ–¼ï¸ validation_predictions/
â”‚ â”œâ”€â”€ val_batch0_pred.jpg
â”‚ â””â”€â”€ â€¦
â”œâ”€â”€ðŸ“„ README.md
â””â”€â”€ðŸ“¦ requirements.txt
```

---

## ðŸ“Š Data Preparation

1. **Dataset Structure**  
   Place `HackByte_Dataset` in `data/`:
```text  
data/
â”œâ”€â”€ train/
â”‚ â”œâ”€â”€ images/
â”‚ â””â”€â”€ labels/
â”œâ”€â”€ val/
â”‚ â”œâ”€â”€ images/
â”‚ â””â”€â”€ labels/
â””â”€â”€ test/
â”œâ”€â”€ images/
â””â”€â”€ labels/
```

2. **Configuration**  
Edit `configs/yolo_params.yaml`:
train: data/train
val: data/val
test: data/test
nc: 3
names: ['FireExtinguisher', 'ToolBox', 'OxygenTank']

---

## ðŸ‹ï¸ Training the Model

Run training with YOLOv8:
python scripts/train.py
--data configs/yolo_params.yaml
--epochs 25
--optimizer SGD
--lr0 0.0005
--mosaic 0.5
--imgsz 640

âœ… Weights & logs â†’ `runs/detect/train/`  
ðŸŽ¯ `best.pt` â†’ highest validation mAP@0.5

---

## ðŸ“ˆ Evaluating the Model

Validate on test set:
from ultralytics import YOLO
model = YOLO('weights/best.pt')
results = model.val(data='configs/yolo_params.yaml', split='test')
print(results.box.map)
ðŸ”§ Or via CLI:
!yolo val model=weights/best.pt data=configs/yolo_params.yaml

---

## ðŸ” Running Inference

Detect on custom images:
python scripts/predict.py
--weights weights/best.pt
--source data/test/images
--conf 0.25
--save
ðŸ”– Outputs â†’ `runs/detect/predict/`

---

## ðŸ”„ Reproducing Final Results

1. Ensure paths in `configs/yolo_params.yaml` match your setup.  
2. Use same hyperparameters & code.  
3. Evaluate with `best.pt` to match reported scores:  
   - **mAP@0.5: 0.915**  
   - **mAP@0.5-0.95: 0.838**

---

## ðŸ“¤ Expected Outputs

- **ðŸ“Š `training_results.png`** â†’ Loss & metrics plots  
- **ðŸŽ¯ `confusion_matrix.png`** â†’ Class confusion  
- **ðŸ“‘ `results.csv`** â†’ Epoch-wise metrics  
- **ðŸ–¼ï¸ `validation_predictions/`** â†’ Sample annotated images

---

## ðŸ§  Interpreting Results

- **mAP@0.5:** Precision @ IoUâ‰¥0.5 (target â‰¥85%)  
- **mAP@0.5-0.95:** Precisions @ varied IoU (target â‰¥75%)  
- **Precision:** Correct detections / total predicted (target â‰¥90%)  
- **Recall:** Correct detections / total actual (target â‰¥80%)  
- **Inference Speed:** Time per image (pre + inf + post)  

> ðŸš€ High metrics confirm robust, real-time detection capabilities!

---

**Â© 2025 Team Cosmic Crushers**  
